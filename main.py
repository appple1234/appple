import streamlit as st
from googleapiclient.discovery import build
import pandas as pd

# --------------------------------------------------
# Streamlit: YouTube ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆœ ì •ë ¬ ì‚¬ì´íŠ¸
# --------------------------------------------------
st.set_page_config(page_title="YouTube ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆœ ì •ë ¬", layout="wide")
st.title("ğŸ” YouTube ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆœìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ì‚¬ì´íŠ¸")

st.write("ìœ íŠœë¸Œ ì˜ìƒ ë§í¬ë¥¼ ì…ë ¥í•˜ë©´ í•´ë‹¹ ì˜ìƒì˜ ëŒ“ê¸€ì„ ë¶ˆëŸ¬ì™€ **ì¢‹ì•„ìš” ë§ì€ ìˆœ**ìœ¼ë¡œ ì •ë ¬í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.")

# -----------------------------
# ì…ë ¥ê°’
# -----------------------------
api_key = st.text_input("YouTube Data API Key", type="password")
video_url = st.text_input("YouTube ì˜ìƒ URL ë˜ëŠ” Video ID")
limit = st.slider("ê°€ì ¸ì˜¬ ëŒ“ê¸€ ìˆ˜ (ìµœëŒ€ 500ê°œ ê¶Œì¥)", 20, 500, 100)
run = st.button("ëŒ“ê¸€ ë¶ˆëŸ¬ì˜¤ê¸°")

# -----------------------------
# Helper: ì˜ìƒ ID ì¶”ì¶œ
# -----------------------------
def extract_video_id(url):
    if "youtube.com/watch?v=" in url:
        return url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url:
        return url.split("youtu.be/")[1].split("?")[0]
    return url

# -----------------------------
# YouTube API ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
# -----------------------------
def get_comments(video_id, key, max_comments=200):
    youtube = build("youtube", "v3", developerKey=key)
    comments = []
    next_page = None
    fetched = 0

    while True:
        req = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page,
            order="relevance"
        )
        res = req.execute()

        for item in res.get("items", []):
            snip = item["snippet"]["topLevelComment"]["snippet"]
            comments.append({
                "author": snip.get("authorDisplayName"),
                "comment": snip.get("textDisplay"),
                "likes": snip.get("likeCount"),
                "published": snip.get("publishedAt"),
            })
            fetched += 1
            if fetched >= max_comments:
                return pd.DataFrame(comments)

        next_page = res.get("nextPageToken")
        if not next_page:
            break

    return pd.DataFrame(comments)

# -----------------------------
# Run
# -----------------------------
if run:
    if not api_key or not video_url:
        st.error("API í‚¤ì™€ ì˜ìƒ ë§í¬ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        vid = extract_video_id(video_url)
        st.write(f"### ğŸ¬ Video ID: `{vid}`")

        df = get_comments(vid, api_key, limit)
        if df is None or df.empty:
            st.error("ëŒ“ê¸€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API ì œí•œ ë˜ëŠ” ëŒ“ê¸€ ì—†ìŒ.")
        else:
            df_sorted = df.sort_values("likes", ascending=False).reset_index(drop=True)

            st.success("ëŒ“ê¸€ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ! ì¢‹ì•„ìš” ìˆœìœ¼ë¡œ ì •ë ¬í–ˆìŠµë‹ˆë‹¤.")

            # Top comment highlight
            top = df_sorted.iloc[0]
            st.markdown(f"""
            ## ğŸ† ê°€ì¥ ì¢‹ì•„ìš” ë§ì€ ëŒ“ê¸€
            **ì‘ì„±ì:** {top['author']}  
            **ì¢‹ì•„ìš”:** {top['likes']} ğŸ‘  
            **ì‘ì„±ì¼:** {top['published']}  

            ---
            {top['comment']}
            """)

            st.write("---")
            st.write("## ğŸ“„ ì „ì²´ ì •ë ¬ëœ ëŒ“ê¸€ ëª©ë¡")
            st.dataframe(df_sorted, use_container_width=True)

            # CSV Export
            st.download_button(
                label="ğŸ“¥ CSVë¡œ ë‹¤ìš´ë¡œë“œ",
                data=df_sorted.to_csv(index=False).encode('utf-8'),
                file_name="youtube_comments_sorted.csv",
                mime="text/csv"
            )
